# -*- coding: utf-8 -*-
"""Parking_slot_detect.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mcKrKllYILjEM3GtbBHzEIkSC2gBH14C
"""

from google.colab import drive
drive.mount('/gdrive')
BASE_PATH ="/gdrive/My Drive/Artificial Intellegence/YOLO/logs/"

! mkdir ./movie
! mkdir ./movie/tmp
! mkdir ./movie/train
! mkdir ./movie/test

TRAIN_MOVIE = "./movie/train/"
TEST_MOVIE = "./movie/test/"
TMP_MOVIE ="./movie/tmp/"

from pytube import YouTube 
import pdb
import cv2 
import pandas as pd
import numpy as np
import gc
from tqdm import tqdm_notebook as tqdm
from datetime import datetime

! mkdir ./video
! mkdir ./images
! mkdir ./model_data

def download_file(link, file) :
  video = "./video/" #to_do 
  yt = YouTube(link)   
  yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').first().download(output_path=video, filename=str(file))

  print(link, ' Downloaded to ',video+ str(file)+".mp4", yt.length,"Sec")

  return video+ str(file)+".mp4"

def FrameCapture(path, index, period = 30): 
  image_path = "./images/"
  vidObj = cv2.VideoCapture(path) 
  fps = vidObj.get(cv2.CAP_PROP_FPS)
  step = int(fps*period)
  count = 0
  success = 1
  files = []
  while success: 
      success, image = vidObj.read() 
      if  count  % step == 0 :
        f = image_path+str(index)+"-"+str(count)+".jpg"
        cv2.imwrite(f, image) 
        files.append(f)
      count += 1
  print("Processed ", path , " @ ", fps, "FPS made ", len(files), "images every", period, "sec" )
  return files

# import imageio
# with imageio.get_writer('/path/to/movie.gif', mode='I') as writer:
#     for filename in filenames:
#         image = imageio.imread(filename)
#         writer.append_data(image)

! git clone https://github.com/qqwweee/keras-yolo3.git
! mv keras-yolo3/*  ./

! wget https://pjreddie.com/media/files/yolov3.weights
# ! python ./convert.py ./yolov3.cfg yolov3.weights model_data/yolo.h5
! rm -rf yolov3.weights

a =1,2
a

#@title
   
# -*- coding: utf-8 -*-
"""
Class definition of YOLO_v3 style detection model on image and video
"""

import colorsys
import os
from timeit import default_timer as timer

import numpy as np
from keras import backend as K
from keras.models import load_model
from keras.layers import Input
from PIL import Image as PILImage
from PIL import ImageFont, ImageDraw

from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body
from yolo3.utils import letterbox_image
import os
from keras.utils import multi_gpu_model
from train_bottleneck import create_model

from PIL import Image as PILImage

import matplotlib.pyplot as plt




# links= ['https://www.youtube.com/watch?v=FaBHiQYTNpw',
#         'https://www.youtube.com/watch?v=U7HRKjlXK-Y',
#         'https://www.youtube.com/watch?v=V2avk-oSAc0'  
# ]

# i =0
# #vid = download_file(links[i], i) 
# vid = "./video/0.mp4"
# img = FrameCapture(vid, i, period = 30)
# data = create_boxes(img)
# slots =  look_for_slots(data)

# slots

! wget http://cnrpark.it/dataset/CNR-EXT_FULL_IMAGE_1000x750.tar
! tar -xvf  CNR-EXT_FULL_IMAGE_1000x750.tar
! rm CNR-EXT_FULL_IMAGE_1000x750.tar

! wget http://cnrpark.it/dataset/CNR-EXT-Patches-150x150.zip
! unzip CNR-EXT-Patches-150x150.zip
! rm CNR-EXT-Patches-150x150.zip

"""! wget http://cnrpark.it/dataset/CNR-EXT-Patches-150x150.zip
! unzip  CNR-EXT-Patches-150x150.zip
! rm CNR-EXT-Patches-150x150.zip
"""

def get_data():
  PATH = "./FULL_IMAGE_1000x750/"
  image_data  =  pd.DataFrame()#[],columns=["image","path","camera","date","condition"])
  conditions = ["SUNNY" , "RAINY", "OVERCAST"]
  dates = os.listdir(PATH)

  for condition in conditions :
    date_path   = PATH+condition+"/"
    dates  = os.listdir(date_path)
    for date in dates : 
      camera_path   = date_path+date+"/"
      cameras  = os.listdir(camera_path)
      for camera in cameras :
        #print(camera)
        images = os.listdir(camera_path+camera+"/")
        paths  =  [camera_path+camera+"/" +  image for image in images]
        d =  pd.DataFrame({"image" : images ,  "path": paths})
        d["camera"]  = camera
        d["date"] = date
        d["condition"] = condition
  #      print(d)
        image_data = image_data.append(d, ignore_index=True)
  image_data["camera"] = image_data["camera"].astype("category")
  image_data["date"] = image_data["date"].astype("category")
  image_data["condition"] = image_data["condition"].astype("category")
  return image_data


image_data =  get_data()

import matplotlib.animation as animation
import numpy as np



      
def plot_frame( image, out_boxes,  out_classes, found, labels):
    image =PILImage.fromarray(cv2.imread(image))
    font = ImageFont.truetype(font='font/FiraMono-Medium.otf',
                size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))
    thickness = (image.size[0] + image.size[1]) // 300
    #colors ={"empty":"#4a148c","occupy":"#f44336", "new":"#7cb342","del":"#80deea"  }
    colors ={"empty":"green","occupy":"red", "new":"blue","del":"grey"  }
    style= {"empty":"--","occupy":"-", "new":"-","del":":"  }
    plt.figure(figsize = (20,20))
    for i, c in list(enumerate(out_classes)):
    # out_classes = [retained, new, deleted]

        box = out_boxes[i]
        label = str(labels[i]) + " (" + str(found[i]) +")"
        draw = ImageDraw.Draw(image)
        label_size = draw.textsize(label, font)

        top, left, bottom, right = box
        top = max(0, np.floor(top + 0.5).astype('int32'))
        left = max(0, np.floor(left + 0.5).astype('int32'))
        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))
        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))
        #print(label, (left, top), (right, bottom))

        if top - label_size[1] >= 0:
            text_origin = np.array([left, top - label_size[1]])
        else:
            text_origin = np.array([left, top + 1])


        for i in range(thickness):
            draw.rectangle(
                [left + i, top + i, right - i, bottom - i],
                outline=colors[c],)# linestyle = style[c])
        draw.rectangle(
            [tuple(text_origin), tuple(text_origin + label_size)],
            fill="white")
        draw.text(text_origin, label, fill=(0, 0, 0), font=font, color="b")
        del draw
#     plt.imshow(image)
#     image = image.resize((image.size[0]//2,image.size[1]//2))
    plt.imshow(image)
    plt.show()
    ts = int(datetime.now().timestamp()*10000)
#     plt.imsave(TMP_MOVIE+str(ts)+".png",image)
    plt.close()
    return image



df  =  pd.read_csv("./LABELS/all.txt" , names = ["name", "label",'Cond', 'date', 'camid', 'file'], sep =" ")
df[["Cond","date","camid","file"]] =  df["name"].str.split("/", n = 4,expand = True) 
df["name"] =  df["name"].apply(lambda x :  "./PATCHES/"+x)
df.head()

df_sh = df[["name", "label"]]
df_sh.head()

from fastai import *
from fastai.vision import *

src= (ImageList.from_df( df_sh,".",) #Where to find the data? -> in path and its subfolders
        .split_by_rand_pct()              #How to split in train/valid? -> use the folders
        .label_from_df(cols='label') )             
                 

data = (src.transform(get_transforms(), size=128)       #Data augmentation? -> use tfms with a size of 64
        .databunch()
        .normalize(imagenet_stats))

learn = cnn_learner(data, models.resnet50,  model_dir='./LABELS/').to_fp16()
learn.load(BASE_PATH+"resnet_cars.h5")

# learn.lr_find()
# learn.recorder.plot()

# lr = 0.01
# learn.fit_one_cycle(2, slice(lr))

# learn.recorder.plot_losses()
# learn.save(BASE_PATH+"resnet_cars.h5")

# interp = ClassificationInterpretation.from_learner(learn)
# interp.plot_confusion_matrix()

def predict(image):
  im = Image(pil2tensor(image,np.float32 ).div_(255))
  log_preds_single = learn.predict(im) # Predict Imag
  return log_preds_single[0].obj

def find_cars_in_slots(park_slots, image, plot=False, k=0) : 
  found = np.zeros(len(park_slots)).astype(int)
  font = ImageFont.truetype(font='font/FiraMono-Medium.otf',
                size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))
  thickness = (image.size[0] + image.size[1]) // 300
  colors =["green","red"]
  
  for i in range(len(park_slots)) : 
    outbox =park_slots.loc[i,["x2","y1","x1","y2"]].values.astype(int)
    crop = image.crop(outbox)
    found[i] = int(predict(crop))
    if plot :
        draw = ImageDraw.Draw(image)
        label =  str(park_slots.at[i,"labels"])
        label_size = draw.textsize(label, font)
        left, top, right, bottom = outbox
#         top, left, bottom, right = box
        top = max(0, np.floor(top + 0.5).astype('int32'))
        left = max(0, np.floor(left + 0.5).astype('int32'))
        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))
        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))
        #print(label, (left, top), (right, bottom))

        if top - label_size[1] >= 0:
            text_origin = np.array([left, top - label_size[1]])
        else:
            text_origin = np.array([left, top + 1])
        for it in range(thickness):
            draw.rectangle(
                [left + it, top + it, right - it, bottom - it], outline=colors[found[i]],)# linestyle = style[c])
        draw.rectangle(
            [tuple(text_origin), tuple(text_origin + label_size)],
            fill="white")
        draw.text(text_origin, label, fill=(0, 0, 0), font=font, color="b")
        del draw
  if plot:
    plt.figure(figsize = (20,20))
    plt.imshow(image)
    plt.show()
#     image = image.resize((image.size[0]//2,image.size[1]//2))
#     ts = int(datetime.now().timestamp()*10000)
#     plt.imsave(TMP_MOVIE+str(ts)+".png",image)
    plt.close()
  return found

yolo = YOLO(score =0.1)

from cv2 import VideoWriter, VideoWriter_fourcc
import imageio


        


def create_video(path ="./movie/tmp/" , file_name="./movie/train/output.mp4"):

  img_array = []
  files = os.listdir(path)
  files = np.sort(files)
  with imageio.get_writer(file_name, mode='I', duration = 2,palettesize =32,subrectangles =True) as writer:
    for i in range(len(files)):
        image = imageio.imread(path+files[i])
        writer.append_data(image)
#         frame = cv2.imread(path+files[i])
#         img_array.append(frame)
        os.remove(path+files[i])
#     height, width, layers = frame.shape
#     size = (width,height)
#     out = cv2.VideoWriter(file_name,cv2.VideoWriter_fourcc(*'DIVX'), 2, size)
#     for i in range(len(img_array)):
#         out.write(img_array[i])
#     out.release()
  writer.close()
  return file_name

def train_and_detect(image_data, camera, pred_fr=120):
  print("PROCESSING", camera)
  images =  image_data[image_data["camera"] == camera ]["path"].values
  images = np.sort(images)

  img_train = images[:len(images) // 2]
  img_pred = images[len(images) // 2:len(images) // 2 +2]
  park_data =  create_boxes(img_train)
  park_slots = look_for_slots(park_data, img= img_train,plot =False,
                                  PRUNE_TH = 1,
                                  PRUNE_STEP =  10,
                                  MERGE_STEP =  50,
                                  MERGE_TH =  0.8)
  park_slots.drop(park_slots[park_slots["found"] < 3].index, inplace=True) 
  park_slots=compute_distance(park_slots, images[20], th=0.2,  label ="20")
  park_slots[['x1', 'y1', 'x2', 'y2',  'xc', 'yc', 'w' , 'b', "found"]] = park_slots[['x1', 'y1', 'x2', 'y2',  'xc', 'yc', 'w' , 'b', "found"]].astype(int)
#   create_video(path ="./movie/tmp/",file_name="./movie/train/"+ camera+"_train.gif" )
  park_slots= park_slots.reset_index(drop=True)
  img_pred_small = img_pred[:pred_fr]
  found = np.zeros((len(img_pred_small),len(park_slots)))
  print("PROCESSING ",pred_fr, "FRAMES TO DETECT OCCUPANCY")
  for i in tqdm(range(len(img_pred_small))):
    image = PILImage.open(img_pred[i]).convert('RGB')
    found[i,:] = find_cars_in_slots(park_slots, image, plot=True, k=i)
#   create_video(path ="./movie/tmp/",file_name="./movie/train/"+ camera+"_detection.gif" ) 
  return park_slots, found.astype(bool)

train_and_detect(image_data, 'camera1', pred_fr=120)

cameras = image_data["camera"].unique()
slots ={}
slot_status ={}
for camera in cameras : 
    park_slots, found = train_and_detect(image_data, camera, pred_fr=120)
    gc.collect()
    slots[camera] = park_slots
    slot_status[camera] = found

gc.collect()
! rm -rf ./movie/tmp/*
! ls -al ./movie/tmp/

# ! mkdir "/gdrive/My Drive/Artificial Intellegence/Parking"
! cp ./movie/train/* "/gdrive/My Drive/Artificial Intellegence/Parking/"

! ls -al "/gdrive/My Drive/Artificial Intellegence/Parking/"