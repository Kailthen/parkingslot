# -*- coding: utf-8 -*-
"""Parking_slot_detect.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mcKrKllYILjEM3GtbBHzEIkSC2gBH14C
"""

! pip install pytube
from google.colab import drive
drive.mount('/gdrive')
BASE_PATH ="/gdrive/My Drive/Artificial Intellegence/YOLO/logs/"

! mkdir ./movie
! mkdir ./movie/tmp
! mkdir ./movie/train
! mkdir ./movie/test

TRAIN_MOVIE = "./movie/train/"
TEST_MOVIE = "./movie/test/"
TMP_MOVIE ="./movie/tmp/"

from pytube import YouTube 
import pdb
import cv2 
import pandas as pd
import numpy as np
import gc
from tqdm import tqdm_notebook as tqdm
from datetime import datetime

! mkdir ./video
! mkdir ./images
! mkdir ./model_data

def download_file(link, file) :
  video = "./video/" #to_do 
  yt = YouTube(link)   
  yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').first().download(output_path=video, filename=str(file))

  print(link, ' Downloaded to ',video+ str(file)+".mp4", yt.length,"Sec")

  return video+ str(file)+".mp4"

def FrameCapture(path, index, period = 30): 
  image_path = "./images/"
  vidObj = cv2.VideoCapture(path) 
  fps = vidObj.get(cv2.CAP_PROP_FPS)
  step = int(fps*period)
  count = 0
  success = 1
  files = []
  while success: 
      success, image = vidObj.read() 
      if  count  % step == 0 :
        f = image_path+str(index)+"-"+str(count)+".jpg"
        cv2.imwrite(f, image) 
        files.append(f)
      count += 1
  print("Processed ", path , " @ ", fps, "FPS made ", len(files), "images every", period, "sec" )
  return files

# import imageio
# with imageio.get_writer('/path/to/movie.gif', mode='I') as writer:
#     for filename in filenames:
#         image = imageio.imread(filename)
#         writer.append_data(image)

! git clone https://github.com/qqwweee/keras-yolo3.git
! mv keras-yolo3/*  ./

! wget https://pjreddie.com/media/files/yolov3.weights
# ! python ./convert.py ./yolov3.cfg yolov3.weights model_data/yolo.h5
! rm -rf yolov3.weights

a =1,2
a

#@title
   
# -*- coding: utf-8 -*-
"""
Class definition of YOLO_v3 style detection model on image and video
"""

import colorsys
import os
from timeit import default_timer as timer

import numpy as np
from keras import backend as K
from keras.models import load_model
from keras.layers import Input
from PIL import Image as PILImage
from PIL import ImageFont, ImageDraw

from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body
from yolo3.utils import letterbox_image
import os
from keras.utils import multi_gpu_model
from train_bottleneck import create_model

class YOLO(object):
    _defaults = {
        "model_path": BASE_PATH+ "trained_weights_final.h5", #
        "anchors_path": 'model_data/yolo_anchors.txt',
        "classes_path": 'model_data/coco_classes.txt',
        "score" : 0.15,
        "iou" : 0.4,
        "model_image_size" : (416, 416),
        "gpu_num" : 1,
    }

    @classmethod
    def get_defaults(cls, n):
        if n in cls._defaults:
            return cls._defaults[n]
        else:
            return "Unrecognized attribute name '" + n + "'"

    def __init__(self, **kwargs):
        self.__dict__.update(self._defaults) # set up default values
        self.__dict__.update(kwargs) # and update with user overrides
        self.class_names = self._get_class()
        self.anchors = self._get_anchors()
        self.sess = K.get_session()
        self.boxes, self.scores, self.classes = self.generate()

    def _get_class(self):
#         classes_path = os.path.expanduser(self.classes_path)
#         with open(classes_path) as f:
#             class_names = f.readlines()
#         class_names = [c.strip() for c in class_names]
        class_names = ["car"]
        return class_names

    def _get_anchors(self):
#         anchors_path = os.path.expanduser(self.anchors_path)
#         with open(anchors_path) as f:
#             anchors = f.readline()
#         anchors = [float(x) for x in anchors.split(',')]
        anchors = [[ 36 , 21],[ 44,  32],[ 59,  56],[ 63,  32],[ 87,  99],[ 92,  45],[149,  70],[221, 128], [413, 237]]
        return np.array(anchors)#.reshape(-1, 2)

    def generate(self):
        model_path = os.path.expanduser(self.model_path)
        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'

        # Load model, or construct model and load weights.
        num_anchors = len(self.anchors)
        num_classes = len(self.class_names)
        is_tiny_version = num_anchors==6 # default setting
        try:
#             pdb.set_trace()
#             self.yolo_model,_,_ = create_model(self.model_image_size, self.anchors, num_classes,
#                   freeze_body=2, weights_path=model_path)
            self.yolo_model = load_model(model_path, compile=False)
            print(self.yolo_model.summary())
        except:
            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \
                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)
            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match
        else:
            assert self.yolo_model.layers[-1].output_shape[-1] == \
                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \
                'Mismatch between model and given anchor and class sizes'

        print('{} model, anchors, and classes loaded.'.format(model_path))

        # Generate colors for drawing bounding boxes.
        hsv_tuples = [(x / len(self.class_names), 1., 1.)
                      for x in range(len(self.class_names))]
        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))
        self.colors = list(
            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),
                self.colors))
        np.random.seed(10101)  # Fixed seed for consistent colors across runs.
        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.
        np.random.seed(None)  # Reset seed to default.

        # Generate output tensor targets for filtered bounding boxes.
        self.input_image_shape = K.placeholder(shape=(2, ))
        if self.gpu_num>=2:
            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)
        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,
                len(self.class_names), self.input_image_shape,
                score_threshold=self.score, iou_threshold=self.iou)
        return boxes, scores, classes
      
    def find_objects(self, image):  
      if self.model_image_size != (None, None):
          assert self.model_image_size[0]%32 == 0, 'Multiples of 32 required'
          assert self.model_image_size[1]%32 == 0, 'Multiples of 32 required'
          boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))
      else:
          new_image_size = (image.width - (image.width % 32),
                            image.height - (image.height % 32))
          boxed_image = letterbox_image(image, new_image_size)
      image_data = np.array(boxed_image, dtype='float32')

      #print(image_data.shape)
      image_data /= 255.
      image_data = np.expand_dims(image_data, 0)  # Add batch dimension.

      out_boxes, out_scores, out_classes = self.sess.run(
          [self.boxes, self.scores, self.classes],
          feed_dict={
              self.yolo_model.input: image_data,
              self.input_image_shape: [image.size[1], image.size[0]],
              K.learning_phase(): 0
          })

      #print('Found {} boxes for {}'.format(len(out_boxes), 'img'))
      return out_boxes, out_scores, out_classes , np.arange(len(out_classes))  
    
    
    def draw_rect(self, image, out_boxes, out_scores, out_classes, labels):
        font = ImageFont.truetype(font='font/FiraMono-Medium.otf',
                    size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))
        thickness = (image.size[0] + image.size[1]) // 300
        im_cnt = 0
        for i, c in reversed(list(enumerate(out_classes))):
            
            predicted_class = self.class_names[c]
            if predicted_class in ["car","bus","truck"] :
              box = out_boxes[i]
              score = out_scores[i]

              label = str(labels[i])#'{}'.format(im_cnt)
              im_cnt= im_cnt+1
              draw = ImageDraw.Draw(image)
              label_size = draw.textsize(label, font)

              top, left, bottom, right = box
              top = max(0, np.floor(top + 0.5).astype('int32'))
              left = max(0, np.floor(left + 0.5).astype('int32'))
              bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))
              right = min(image.size[0], np.floor(right + 0.5).astype('int32'))
              #print(label, (left, top), (right, bottom))

              if top - label_size[1] >= 0:
                  text_origin = np.array([left, top - label_size[1]])
              else:
                  text_origin = np.array([left, top + 1])

              # My kingdom for a good redistributable image drawing library.
              for i in range(thickness):
                  draw.rectangle(
                      [left + i, top + i, right - i, bottom - i],
                      outline=self.colors[c])
              draw.rectangle(
                  [tuple(text_origin), tuple(text_origin + label_size)],
                  fill=self.colors[c])
              draw.text(text_origin, label, fill=(0, 0, 0), font=font)
              del draw
        return image


    def detect_image(self, image, draw = True):
        start = timer()

        out_boxes, out_scores, out_classes, labels = self.find_objects(image)

        
        end = timer()
        self.draw_rect( image, out_boxes, out_scores, out_classes, labels)
        #print(end - start)
        return image

    def close_session(self):
        self.sess.close()

def detect_video(yolo, video_path, output_path=""):
    import cv2
    vid = cv2.VideoCapture(video_path)
    if not vid.isOpened():
        raise IOError("Couldn't open webcam or video")
    video_FourCC    = int(vid.get(cv2.CAP_PROP_FOURCC))
    video_fps       = vid.get(cv2.CAP_PROP_FPS)
    video_size      = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),
                        int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))
    isOutput = True if output_path != "" else False
    if isOutput:
        print("!!! TYPE:", type(output_path), type(video_FourCC), type(video_fps), type(video_size))
        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)
    accum_time = 0
    curr_fps = 0
    fps = "FPS: ??"
    prev_time = timer()
    while True:
        return_value, frame = vid.read()
        image =PILImage.fromarray(frame)
        image = yolo.detect_image(image)
        result = np.asarray(image)
        curr_time = timer()
        exec_time = curr_time - prev_time
        prev_time = curr_time
        accum_time = accum_time + exec_time
        curr_fps = curr_fps + 1
        if accum_time > 1:
            accum_time = accum_time - 1
            fps = "FPS: " + str(curr_fps)
            curr_fps = 0
        cv2.putText(result, text=fps, org=(3, 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX,
                    fontScale=0.50, color=(255, 0, 0), thickness=2)
        cv2.namedWindow("result", cv2.WINDOW_NORMAL)
        cv2.imshow("result", result)
        if isOutput:
            out.write(result)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    yolo.close_session()

import matplotlib.pyplot as plt


def plot_detection(images,index = 0 , figsize=(33,64)):
  photo = cv2.imread(images[index])
  detected = yolo.detect_image(PILImage.fromarray(photo))
  f = plt.figure(figsize=figsize)
  sp = f.add_subplot(1, 2, 1)
  sp.axis('Off')
  plt.imshow(photo)
  sp = f.add_subplot(1, 2, 2)
  sp.axis('Off')
  plt.imshow(detected)

def create_boxes(images):
  data  = pd.DataFrame()#[],columns =["x1","y1","x2","y2", "score","class","file"])
  for i in range(len(images)):
    out_boxes, out_scores, out_classes, labels = yolo.find_objects(PILImage.open(images[i]))
    out_boxes = out_boxes.reshape((len(labels),4))
    df  = pd.DataFrame(out_boxes , columns =["y1","x2","y2","x1"])
    
#     {"y1":out_boxes,"x2","y2","x1"}
    df["score"] = out_scores
    df["class"] =out_classes
    df["frame"] = i
    df["labels"] = np.arange(len(df))
    data = data.append(df, ignore_index=True)
    
  data["xc"] = (data["x1"] + data["x2"])/2
  data["yc"] = (data["y1"] + data["y2"])/2
  data["w"] =  data["x1"] -data["x2"]
  data["b"] = data["y2"] - data["y1"]
  data["d"] =  np.sqrt(data["b"]*data["b"]  + data["w"]*data["w"] )
  data = data[["labels", 'x1', 'y1', 'x2', 'y2',  'xc', 'yc', 'w',  'd' , 'b', 'score', 'class', 'frame' ]]
#   mask = data["class"].apply(lambda x: x in [2, 5, 7]) 
#   data = data[mask]
  return data



pd.options.mode.chained_assignment = None # Disable warning from pandas

def calc_iou(x1,y1,x2,y2,df):
  df = df.reset_index(drop=True)
  ar_df = (df["w"]*df["b"]).values
  ar    = (x1-x2)*(y2-y1)
  int_ar =  np.zeros(len(df))
  for i in range(len(df)):
    dx1 = df.at[i,"x1"]
    dy1 = df.at[i,"y1"]
    dx2 = df.at[i,"x2"]
    dy2 = df.at[i,"y2"]
    
    dx = min(dx1, x1) - max(dx2, x2)
    dy = min(dy2, y2) - max(dy1, y1) 
    #pdb.set_trace()
    if (dx>=0) and (dy>=0):
        int_ar[i] = dx*dy
  mask =  np.logical_or(int_ar >= ar_df ,  int_ar >= ar)
  
  
  iou = int_ar /(ar + ar_df - int_ar)
  iou[mask] =1
  return iou

def assign_next_frame(prior, post, th = 0.7, pr =False):
  iou =np.zeros(len(prior))
  status =np.zeros(len(prior))
  iou_mat = np.zeros((len(prior), len(post)))
  for k in range(len(prior)) : 
    if pr  and k ==18:
      pdb.set_trace()
      print(k,len(prior))
    p = prior.loc[k,:]
    iou_mat[k,:] = calc_iou(p.x1,p.y1,p.x2,p.y2, post)
  #iou_mat =  np.tril(iou_mat)
  id_map ={}
  count =  min(len(prior), len(post))
  mat=np.copy(iou_mat)
  while count >0 :
    #pdb.set_trace()
    r,k  = np.unravel_index(np.argmax(iou_mat, axis=None), iou_mat.shape)
    if iou_mat[r,k] > th :
      id_map[post.at[k,"labels"]] =prior.at[r,"labels"]
      iou[r] = iou_mat[r,k]
      status[r]=1
    iou_mat[r,:] =  -99
    iou_mat[:,k] =  -99
    count = count -1
  return mat, iou, id_map, status.astype(bool)

def compute_distance(df, image, th = 0.92, label = "Parking Slots", plot = False):
  df.reset_index(drop=True, inplace=True)
  n =  len(df)
  base_col = ['x1', 'y1', 'x2', 'y2',  'xc', 'yc', 'w' , 'b']
  df.reset_index(drop=True, inplace=True)
  mat, _, _, _ = assign_next_frame(df, df, th = 0.6)
  np.fill_diagonal(mat, -9)
  mat = np.tril(mat)
  count = n
  to_merge = []
  while count > 0:
    r,k = np.unravel_index(np.argmax(mat, axis=None), mat.shape)
    if mat[r,k] > th :
      to_merge.append([r,k])

    mat[r,:] = -9
    mat[:,k] = -9
    mat[k,:] = -9
    mat[:,r] = -9
    count = count -1
#   print(to_merge)
  for i in range(len(to_merge)):
    r = to_merge[i][0]
    k = to_merge[i][1]
    df.loc[r,base_col] =(df.loc[r,base_col] * df.loc[r,"found"] +  df.loc[r,base_col]* df.loc[k,"found"])/(df.loc[r,"found"]+df.loc[k,"found"])
    df.at[r,"found"] =  df.at[r,"found"]+ df.at[k,"found"]
    df.drop(k, axis=0, inplace = True)
  if plot :
    plt.figure()
    box_img = yolo.draw_rect(Image.fromarray(cv2.imread(image)),\
           df[["y1","x2","y2","x1"]].values, df.index.values, df["class"], df["labels"].values)
    f = plt.figure(figsize=(20,40))
    plt.imshow(box_img)
    plt.title(label)
    plt.show()
    plt.close()
  return df
#   dist_mat = np.zeros(n,n)
#   for in in range(len(df)):
#     dist_mat

def look_for_slots(data, img=[],PRUNE_TH = 3, plot = True,
                                PRUNE_STEP =  10,
                                MERGE_STEP =  20,
                                MERGE_TH =  0.75):
  
  
  n_fr = data["frame"].nunique()
  cols = ["labels", 'x1', 'y1', 'x2', 'y2',  'xc', 'yc', 'w' , 'b',"class" ]
  base_col = ['x1', 'y1', 'x2', 'y2',  'xc', 'yc', 'w' , 'b']
  slots  = data[data["frame"] == 0 ][cols]
  slots["found"] = 1

#   out_boxes,  out_classes, found, labels
# "empty":"#4a148c","occupy":"#f44336", "new":"#7cb342","del":"#80deea" 
  print("LOOKING FOR PARKING SLOTS INSIDE IMAGE FRAMES")
  for i in  tqdm(range(1 ,n_fr)) : 
    post =  data[data["frame"]==i].reset_index(drop=True)
    _,iou, id_map, status = assign_next_frame(slots, post, th = 0.6)
    #print(id_map.keys(), status.sum())
    
    ## found again
    mask = post["labels"].isin(id_map.keys())
    slots.loc[status,"found"] = slots.loc[status,"found"] +1
    occupy =  post[mask]
    occupy["labels"] = occupy["labels"].map(id_map)
    slots.sort_values(by =["labels"] , inplace=True)
    slots.reset_index(drop=True, inplace=True)
    occupy.sort_values(by =["labels"], inplace = True)
    occupy.reset_index(drop=True, inplace=True)
    slots.loc[status,base_col] = slots.loc[status,base_col].values *(1 - 1/(i+1)) +  occupy[base_col].values/(i+1)
     
    # clean up
    if i % PRUNE_STEP ==0 :
      slots.drop(slots[slots["found"] < PRUNE_TH+1].index, inplace=True) 
      #print(slots)

    # merge 
    if i % MERGE_STEP ==0 :
      
      slots = compute_distance(slots, img[i-1], th = MERGE_TH, label = "Parking Slots "+ str(i))
       
    # new
    idx = np.logical_not(post["labels"].isin(id_map.keys()))
    new  =  post[idx]
    new["labels"] =  new["labels"] + slots["labels"].max() + 1
    new  = new[cols]
    if len(new ) > 0 :
      new["found"] = 1
    slots = slots.append(new,  sort=True).reset_index(drop=True) 
    if plot | (i % MERGE_STEP*5 ==0):
      df = slots[['x1', 'y1', 'x2', 'y2',"found","labels" ]]
      df["class"] ="empty"
      msk = df["labels"].isin(id_map.values())
      df.loc[msk,"class"] = "occupy"
      nw_df = new[['x1', 'y1', 'x2', 'y2',"labels" ]]
      nw_df["class"] = "new"
      nw_df["found"]=1
      df = df.append(nw_df,  sort=True).reset_index(drop=True)
      df["found"] = df["found"].astype(int)
      plot_frame( img[i], df[["y1","x2","y2","x1"]].values,  df["class"].values, df["found"], df["labels"])
    
  slots.drop(slots[slots["found"] < PRUNE_TH*3].index, inplace=True) 
  slots = compute_distance(slots, img[0], th = MERGE_TH*0.8, label = "Parking Slots "+ str(MERGE_STEP))
  print(len(slots), "SLOTS FOUND")
  return slots

# links= ['https://www.youtube.com/watch?v=FaBHiQYTNpw',
#         'https://www.youtube.com/watch?v=U7HRKjlXK-Y',
#         'https://www.youtube.com/watch?v=V2avk-oSAc0'  
# ]

# i =0
# #vid = download_file(links[i], i) 
# vid = "./video/0.mp4"
# img = FrameCapture(vid, i, period = 30)
# data = create_boxes(img)
# slots =  look_for_slots(data)

# slots

! wget http://cnrpark.it/dataset/CNR-EXT_FULL_IMAGE_1000x750.tar
! tar -xvf  CNR-EXT_FULL_IMAGE_1000x750.tar
! rm CNR-EXT_FULL_IMAGE_1000x750.tar

! wget http://cnrpark.it/dataset/CNR-EXT-Patches-150x150.zip
! unzip CNR-EXT-Patches-150x150.zip
! rm CNR-EXT-Patches-150x150.zip

"""! wget http://cnrpark.it/dataset/CNR-EXT-Patches-150x150.zip
! unzip  CNR-EXT-Patches-150x150.zip
! rm CNR-EXT-Patches-150x150.zip
"""

def get_data():
  PATH = "./FULL_IMAGE_1000x750/"
  image_data  =  pd.DataFrame()#[],columns=["image","path","camera","date","condition"])
  conditions = ["SUNNY" , "RAINY", "OVERCAST"]
  dates = os.listdir(PATH)

  for condition in conditions :
    date_path   = PATH+condition+"/"
    dates  = os.listdir(date_path)
    for date in dates : 
      camera_path   = date_path+date+"/"
      cameras  = os.listdir(camera_path)
      for camera in cameras :
        #print(camera)
        images = os.listdir(camera_path+camera+"/")
        paths  =  [camera_path+camera+"/" +  image for image in images]
        d =  pd.DataFrame({"image" : images ,  "path": paths})
        d["camera"]  = camera
        d["date"] = date
        d["condition"] = condition
  #      print(d)
        image_data = image_data.append(d, ignore_index=True)
  image_data["camera"] = image_data["camera"].astype("category")
  image_data["date"] = image_data["date"].astype("category")
  image_data["condition"] = image_data["condition"].astype("category")
  return image_data


image_data =  get_data()

import matplotlib.animation as animation
import numpy as np



      
def plot_frame( image, out_boxes,  out_classes, found, labels):
    image =PILImage.fromarray(cv2.imread(image))
    font = ImageFont.truetype(font='font/FiraMono-Medium.otf',
                size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))
    thickness = (image.size[0] + image.size[1]) // 300
    #colors ={"empty":"#4a148c","occupy":"#f44336", "new":"#7cb342","del":"#80deea"  }
    colors ={"empty":"green","occupy":"red", "new":"blue","del":"grey"  }
    style= {"empty":"--","occupy":"-", "new":"-","del":":"  }
    plt.figure(figsize = (20,20))
    for i, c in list(enumerate(out_classes)):
    # out_classes = [retained, new, deleted]

        box = out_boxes[i]
        label = str(labels[i]) + " (" + str(found[i]) +")"
        draw = ImageDraw.Draw(image)
        label_size = draw.textsize(label, font)

        top, left, bottom, right = box
        top = max(0, np.floor(top + 0.5).astype('int32'))
        left = max(0, np.floor(left + 0.5).astype('int32'))
        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))
        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))
        #print(label, (left, top), (right, bottom))

        if top - label_size[1] >= 0:
            text_origin = np.array([left, top - label_size[1]])
        else:
            text_origin = np.array([left, top + 1])


        for i in range(thickness):
            draw.rectangle(
                [left + i, top + i, right - i, bottom - i],
                outline=colors[c],)# linestyle = style[c])
        draw.rectangle(
            [tuple(text_origin), tuple(text_origin + label_size)],
            fill="white")
        draw.text(text_origin, label, fill=(0, 0, 0), font=font, color="b")
        del draw
#     plt.imshow(image)
#     image = image.resize((image.size[0]//2,image.size[1]//2))
    plt.imshow(image)
    plt.show()
    ts = int(datetime.now().timestamp()*10000)
#     plt.imsave(TMP_MOVIE+str(ts)+".png",image)
    plt.close()
    return image



df  =  pd.read_csv("./LABELS/all.txt" , names = ["name", "label",'Cond', 'date', 'camid', 'file'], sep =" ")
df[["Cond","date","camid","file"]] =  df["name"].str.split("/", n = 4,expand = True) 
df["name"] =  df["name"].apply(lambda x :  "./PATCHES/"+x)
df.head()

df_sh = df[["name", "label"]]
df_sh.head()

from fastai import *
from fastai.vision import *

src= (ImageList.from_df( df_sh,".",) #Where to find the data? -> in path and its subfolders
        .split_by_rand_pct()              #How to split in train/valid? -> use the folders
        .label_from_df(cols='label') )             
                 

data = (src.transform(get_transforms(), size=128)       #Data augmentation? -> use tfms with a size of 64
        .databunch()
        .normalize(imagenet_stats))

learn = cnn_learner(data, models.resnet50,  model_dir='./LABELS/').to_fp16()
learn.load(BASE_PATH+"resnet_cars.h5")

# learn.lr_find()
# learn.recorder.plot()

# lr = 0.01
# learn.fit_one_cycle(2, slice(lr))

# learn.recorder.plot_losses()
# learn.save(BASE_PATH+"resnet_cars.h5")

# interp = ClassificationInterpretation.from_learner(learn)
# interp.plot_confusion_matrix()

def predict(image):
  im = Image(pil2tensor(image,np.float32 ).div_(255))
  log_preds_single = learn.predict(im) # Predict Imag
  return log_preds_single[0].obj

def find_cars_in_slots(park_slots, image, plot=False, k=0) : 
  found = np.zeros(len(park_slots)).astype(int)
  font = ImageFont.truetype(font='font/FiraMono-Medium.otf',
                size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))
  thickness = (image.size[0] + image.size[1]) // 300
  colors =["green","red"]
  
  for i in range(len(park_slots)) : 
    outbox =park_slots.loc[i,["x2","y1","x1","y2"]].values.astype(int)
    crop = image.crop(outbox)
    found[i] = int(predict(crop))
    if plot :
        draw = ImageDraw.Draw(image)
        label =  str(park_slots.at[i,"labels"])
        label_size = draw.textsize(label, font)
        left, top, right, bottom = outbox
#         top, left, bottom, right = box
        top = max(0, np.floor(top + 0.5).astype('int32'))
        left = max(0, np.floor(left + 0.5).astype('int32'))
        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))
        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))
        #print(label, (left, top), (right, bottom))

        if top - label_size[1] >= 0:
            text_origin = np.array([left, top - label_size[1]])
        else:
            text_origin = np.array([left, top + 1])
        for it in range(thickness):
            draw.rectangle(
                [left + it, top + it, right - it, bottom - it], outline=colors[found[i]],)# linestyle = style[c])
        draw.rectangle(
            [tuple(text_origin), tuple(text_origin + label_size)],
            fill="white")
        draw.text(text_origin, label, fill=(0, 0, 0), font=font, color="b")
        del draw
  if plot:
    plt.figure(figsize = (20,20))
    plt.imshow(image)
    plt.show()
#     image = image.resize((image.size[0]//2,image.size[1]//2))
#     ts = int(datetime.now().timestamp()*10000)
#     plt.imsave(TMP_MOVIE+str(ts)+".png",image)
    plt.close()
  return found

yolo = YOLO(score =0.1)

from cv2 import VideoWriter, VideoWriter_fourcc
import imageio


        


def create_video(path ="./movie/tmp/" , file_name="./movie/train/output.mp4"):

  img_array = []
  files = os.listdir(path)
  files = np.sort(files)
  with imageio.get_writer(file_name, mode='I', duration = 2,palettesize =32,subrectangles =True) as writer:
    for i in range(len(files)):
        image = imageio.imread(path+files[i])
        writer.append_data(image)
#         frame = cv2.imread(path+files[i])
#         img_array.append(frame)
        os.remove(path+files[i])
#     height, width, layers = frame.shape
#     size = (width,height)
#     out = cv2.VideoWriter(file_name,cv2.VideoWriter_fourcc(*'DIVX'), 2, size)
#     for i in range(len(img_array)):
#         out.write(img_array[i])
#     out.release()
  writer.close()
  return file_name

def train_and_detect(image_data, camera, pred_fr=120):
  print("PROCESSING", camera)
  images =  image_data[image_data["camera"] == camera ]["path"].values
  images = np.sort(images)

  img_train = images[:len(images) // 2]
  img_pred = images[len(images) // 2:len(images) // 2 +2]
  park_data =  create_boxes(img_train)
  park_slots = look_for_slots(park_data, img= img_train,plot =False,
                                  PRUNE_TH = 1,
                                  PRUNE_STEP =  10,
                                  MERGE_STEP =  50,
                                  MERGE_TH =  0.8)
  park_slots.drop(park_slots[park_slots["found"] < 3].index, inplace=True) 
  park_slots=compute_distance(park_slots, images[20], th=0.2,  label ="20")
  park_slots[['x1', 'y1', 'x2', 'y2',  'xc', 'yc', 'w' , 'b', "found"]] = park_slots[['x1', 'y1', 'x2', 'y2',  'xc', 'yc', 'w' , 'b', "found"]].astype(int)
#   create_video(path ="./movie/tmp/",file_name="./movie/train/"+ camera+"_train.gif" )
  park_slots= park_slots.reset_index(drop=True)
  img_pred_small = img_pred[:pred_fr]
  found = np.zeros((len(img_pred_small),len(park_slots)))
  print("PROCESSING ",pred_fr, "FRAMES TO DETECT OCCUPANCY")
  for i in tqdm(range(len(img_pred_small))):
    image = PILImage.open(img_pred[i]).convert('RGB')
    found[i,:] = find_cars_in_slots(park_slots, image, plot=True, k=i)
#   create_video(path ="./movie/tmp/",file_name="./movie/train/"+ camera+"_detection.gif" ) 
  return park_slots, found.astype(bool)

train_and_detect(image_data, 'camera1', pred_fr=120)

cameras = image_data["camera"].unique()
slots ={}
slot_status ={}
for camera in cameras : 
    park_slots, found = train_and_detect(image_data, camera, pred_fr=120)
    gc.collect()
    slots[camera] = park_slots
    slot_status[camera] = found

gc.collect()
! rm -rf ./movie/tmp/*
! ls -al ./movie/tmp/

# ! mkdir "/gdrive/My Drive/Artificial Intellegence/Parking"
! cp ./movie/train/* "/gdrive/My Drive/Artificial Intellegence/Parking/"

! ls -al "/gdrive/My Drive/Artificial Intellegence/Parking/"